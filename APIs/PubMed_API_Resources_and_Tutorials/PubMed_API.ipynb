{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b96e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and Import Packages.\n",
    "import sys\n",
    "from Bio import Entrez as ez\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e720181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiopythonDeprecationWarning: Biopython deprecation warning.\n",
      "HTTPError: Raised when HTTP error occurs, but also acts like non-error return\n",
      "Request: No description available.\n",
      "URLError: No description available.\n",
      "ecitmatch: Retrieve PMIDs for input citation strings, returned as a handle.\n",
      "efetch: Fetch Entrez results which are returned as a handle.\n",
      "egquery: Provide Entrez database counts for a global search (DEPRECATED).\n",
      "einfo: Return a summary of the Entrez databases as a results handle.\n",
      "elink: Check for linked external articles and return a handle.\n",
      "epost: Post a file of identifiers for future use.\n",
      "esearch: Run an Entrez search and return a handle to the results.\n",
      "espell: Retrieve spelling suggestions as a results handle.\n",
      "esummary: Retrieve document summaries as a results handle.\n",
      "function_with_previous: Decorate a function as having an attribute named 'previous'.\n",
      "parse: Parse an XML file from the NCBI Entrez Utilities into python objects.\n",
      "read: Parse an XML file from the NCBI Entrez Utilities into python objects.\n",
      "urlencode: Encode a dict or sequence of two-element tuples into a URL query string.\n",
      "urlopen: Open the URL url, which can be either a string or a Request object.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ez_functions = [func for func in dir(ez) if callable(getattr(ez, func)) and not func.startswith('_')]\n",
    "\n",
    "# Loop through each function name in your list\n",
    "for func_name in ez_functions:\n",
    "    # Get the actual function object from the 'ez' module\n",
    "    function_object = getattr(ez, func_name)\n",
    "    \n",
    "    # Get the docstring, handle cases where it might be empty\n",
    "    docstring = function_object.__doc__\n",
    "    if docstring:\n",
    "        # Split the docstring into lines and take the first non-empty one\n",
    "        first_line = docstring.strip().split('\\n')[0]\n",
    "        print(f\"{func_name}: {first_line}\")\n",
    "    else:\n",
    "        print(f\"{func_name}: No description available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98247a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate+change\n",
      "gnasser@charlotte.edu\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "ez.email = os.environ.get('EMAIL')\n",
    "# ez.api_key = os.environ.get('API_KEY')\n",
    "SEARCH_QUERY = os.environ.get('SEARCH_QUERY')\n",
    "DATABASE = os.environ.get('DATABASE')\n",
    "RETTYPE = os.environ.get('RETTYPE')\n",
    "RETMODE = os.environ.get('RETMODE')\n",
    "\n",
    "print(SEARCH_QUERY)\n",
    "print(ez.email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e3281e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate+change\n",
      "Running esearch...\n",
      "Found 116586 results.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(SEARCH_QUERY)\n",
    "\n",
    "print(\"Running esearch...\")\n",
    "handle = ez.esearch(db=DATABASE,\n",
    "                        term=SEARCH_QUERY,\n",
    "                        usehistory=\"y\") # IMPORTANT: Use the history server\n",
    "\n",
    "search_results = ez.read(handle)\n",
    "handle.close()\n",
    "\n",
    "# Get the total count and the history server identifiers\n",
    "count = int(search_results[\"Count\"])\n",
    "webenv = search_results[\"WebEnv\"]\n",
    "query_key = search_results[\"QueryKey\"]\n",
    "\n",
    "print(f\"Found {count} results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8dfafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching records in batches...\n",
      "Fetching records from 1 to 100\n",
      "Fetching records from 101 to 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m fetch_handle \u001b[38;5;241m=\u001b[39m ez\u001b[38;5;241m.\u001b[39mefetch(db \u001b[38;5;241m=\u001b[39m DATABASE,\n\u001b[0;32m     10\u001b[0m                              rettype \u001b[38;5;241m=\u001b[39m RETTYPE, \u001b[38;5;66;03m# Request XML format for full data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m                              retmode\u001b[38;5;241m=\u001b[39m RETMODE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m                              webenv\u001b[38;5;241m=\u001b[39mwebenv, \u001b[38;5;66;03m# Use the history server identifiers\u001b[39;00m\n\u001b[0;32m     15\u001b[0m                              query_key\u001b[38;5;241m=\u001b[39mquery_key)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Biopython's ez.read can parse the XML into a structured Python object\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m records \u001b[38;5;241m=\u001b[39m ez\u001b[38;5;241m.\u001b[39mread(fetch_handle)\n\u001b[0;32m     19\u001b[0m fetch_handle\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# The actual articles are in the 'PubmedArticle' list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\site-packages\\Bio\\Entrez\\__init__.py:529\u001b[0m, in \u001b[0;36mread\u001b[1;34m(source, validate, escape, ignore_errors)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataHandler\n\u001b[0;32m    528\u001b[0m handler \u001b[38;5;241m=\u001b[39m DataHandler(validate, escape, ignore_errors)\n\u001b[1;32m--> 529\u001b[0m record \u001b[38;5;241m=\u001b[39m handler\u001b[38;5;241m.\u001b[39mread(source)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m record\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\site-packages\\Bio\\Entrez\\Parser.py:405\u001b[0m, in \u001b[0;36mDataHandler.read\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile should be opened in binary mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mParseFile(stream)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m expat\u001b[38;5;241m.\u001b[39mExpatError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mStartElementHandler:\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;66;03m# We saw the initial <!xml declaration, so we can be sure that\u001b[39;00m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;66;03m# we are parsing XML data. Most likely, the XML file is\u001b[39;00m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;66;03m# corrupted.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\http\\client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_chunked(amt)\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\http\\client.py:597\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    595\u001b[0m value \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (chunk_left \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_chunk_left()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_left:\n\u001b[0;32m    599\u001b[0m             value\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(amt))\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\http\\client.py:579\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_next_chunk_size()\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\http\\client.py:539\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot read from timed out object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\ssl.py:1304\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1302\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1303\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\Lib\\ssl.py:1138\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "all_records = []\n",
    "\n",
    "print(\"Fetching records in batches...\")\n",
    "for start in range(0, count, batch_size):\n",
    "    end = min(count, start + batch_size)\n",
    "    print(f\"Fetching records from {start+1} to {end}\")\n",
    "    \n",
    "    fetch_handle = ez.efetch(db = DATABASE,\n",
    "                                 rettype = RETTYPE, # Request XML format for full data\n",
    "                                 retmode= RETMODE,\n",
    "                                 retstart=start,\n",
    "                                 retmax=batch_size,\n",
    "                                 webenv=webenv, # Use the history server identifiers\n",
    "                                 query_key=query_key)\n",
    "    \n",
    "    # Biopython's ez.read can parse the XML into a structured Python object\n",
    "    records = ez.read(fetch_handle)\n",
    "    fetch_handle.close()\n",
    "    \n",
    "    # The actual articles are in the 'PubmedArticle' list\n",
    "    all_records.extend(records['PubmedArticle'])\n",
    "    \n",
    "    # Be nice to the server! A short delay between requests.\n",
    "    sleep(0.3)\n",
    "\n",
    "print(\"Finished fetching all records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68b17327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed and saved results to systematic_review_results.csv\n",
      "       PMID                                              Title  \\\n",
      "0  40974187  Exploring Multi-Omics Tools and Their Advancem...   \n",
      "1  40974145  Selenium-Containing Peptides from Foodstuff: P...   \n",
      "2  40974075  The utility of artificial intelligence in the ...   \n",
      "3  40974058  Complex effects of climatic variation on bumbl...   \n",
      "4  40974010  Genome-Scale Metabolic Models in Plant Stress ...   \n",
      "\n",
      "                                            Abstract  Year  \\\n",
      "0  Climate change poses significant threats to op...  2025   \n",
      "1  Selenium-containing peptides (SePPs) are bioac...  2025   \n",
      "2  Dengue fever remains a significant public heal...  2025   \n",
      "3  Climate change is a global biodiversity threat...  2025   \n",
      "4  Global climate change will result in plants be...  2025   \n",
      "\n",
      "                                             Authors  \\\n",
      "0  Kaberi Sonowal; Sonal Sharma; Gokul Anil Kumar...   \n",
      "1  Lingyun Gu; Wenzhu Zhao; Jucheng Hu; Mok Wen J...   \n",
      "2  Mohammed Hussein; Elshimaa Ali; Yassin Kamal; ...   \n",
      "3  Ruth Archer; Paul Schmid-Hempel; Regula Schmid...   \n",
      "4  Érica Mangaravite; Christina Cleo Vinson; Edua...   \n",
      "\n",
      "                                             Journal  \n",
      "0                              Physiologia plantarum  \n",
      "1         Journal of agricultural and food chemistry  \n",
      "2  Transactions of the Royal Society of Tropical ...  \n",
      "3                      The Journal of animal ecology  \n",
      "4                                   Plant physiology  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assuming 'all_records' is a list of dictionaries you've already loaded.\n",
    "# all_records = ... \n",
    "\n",
    "parsed_articles = []\n",
    "\n",
    "for article in all_records:\n",
    "    citation = article['MedlineCitation']\n",
    "    \n",
    "    # FIX: Define article_info here to access the nested 'Article' dictionary\n",
    "    article_info = citation.get('Article', {})\n",
    "\n",
    "    # Use article_info to access all subsequent fields\n",
    "    title = article_info.get('ArticleTitle', 'No Title Found')\n",
    "\n",
    "    abstract_parts = article_info.get('Abstract', {}).get('AbstractText', [])\n",
    "    abstract = ' '.join(abstract_parts) if abstract_parts else 'No Abstract Found'\n",
    "\n",
    "    journal_info = article_info.get('Journal', {})\n",
    "    journal_name = journal_info.get('Title', 'No Journal Found')\n",
    "    \n",
    "    pub_date = journal_info.get('JournalIssue', {}).get('PubDate', {})\n",
    "    year = pub_date.get('Year', pub_date.get('MedlineDate', 'No Year Found'))\n",
    "\n",
    "    pmid = citation.get('PMID', '')\n",
    "\n",
    "    # Now this line will work correctly because article_info is defined\n",
    "    author_list = article_info.get('AuthorList', [])\n",
    "    authors = []\n",
    "    for author_data in author_list:\n",
    "        fore_name = author_data.get('ForeName', '')\n",
    "        last_name = author_data.get('LastName', '')\n",
    "        if fore_name and last_name:\n",
    "            authors.append(f\"{fore_name} {last_name}\")\n",
    "    \n",
    "    authors_str = '; '.join(authors) if authors else 'No Authors Found'\n",
    "\n",
    "    parsed_articles.append({\n",
    "        'PMID': str(pmid),\n",
    "        'Title': title,\n",
    "        'Authors': authors_str,\n",
    "        'Abstract': abstract,\n",
    "        'Journal': journal_name,\n",
    "        'Year': year\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(parsed_articles)\n",
    "\n",
    "# Optional: Reorder columns for a more logical layout in the CSV\n",
    "if not df.empty:\n",
    "    df = df[['PMID', 'Title', 'Abstract', 'Year', 'Authors', 'Journal']]\n",
    "    \n",
    "df.to_csv(\"systematic_review_results.csv\", index=False)\n",
    "\n",
    "print(\"Successfully parsed and saved results to systematic_review_results.csv\")\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
